{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d366922",
   "metadata": {},
   "source": [
    "# Notebook 05 - Loss Functions\n",
    "\n",
    "__Last notebook we ended off training our neural net on a small dataset by writing the following training loop__\n",
    "\n",
    "1) Feed our input samples into the model and perform a forward pass to get the predicted values\n",
    "\n",
    "2) Calculate the loss value (which we want to minimize through training). For this example we'll use a simple maximum-margin hinge loss for binary classification. This is basically squared error where the labels are `-1` or `1`.\n",
    "\n",
    "3) \"Zero out\" the gradients - if we don't manually set the gradients to zero, the gradients for each Scalar value will keep accumulating during model training, which may lead to undesired behaviour during training. The PyTorch equivalent is `zero_grad()`.\n",
    "\n",
    "4) Perform a backward pass to calculate the gradients of every parameter (weight & bias) in the model\n",
    "\n",
    "5) Update the parameter values of the model, each parameter will be adjusted by a `learning_rate` (set to 0.1) multiplied by the negative of the gradient. We'll go deeper into gradient descent next notebook.\n",
    "\n",
    "__In step 2, I kind of just threw a random loss function at you__, so in this notebook we'll be diving deeper into this step and building more functionality around our `Sequential` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189ea7a",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "Going back to notebook 1 for a second, remember a function maps inputs to outputs.\n",
    "\n",
    "A loss function is a function that takes the __predicted output__ of our neural network and the __actual values__ from our training data as inputs, and outputs a __measure of difference between predicted and actual values__, called the __loss value__.\n",
    "\n",
    "This is useful when it comes to neural networks because we want to get our predicted outputs to match the actual values closely, or in other words, __minimize the loss value__. We can achieve this during our training using gradient descent.\n",
    "\n",
    "Since we can decompose functions into basic operations, it means loss functions are differentiable, and by calling `backward()` on the loss value, we can calculate the gradients of the whole network relative to to the loss value through backpropogation, and train the network by updating the weights.\n",
    "\n",
    "In this notebook we'll implement the 3 most common loss functions:\n",
    "\n",
    "- Mean Squared Error (Regression)\n",
    "- Binary Cross Entropy (Binary Classification)\n",
    "- Categorical Cross Entropy (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05403c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "from kaitorch.utils import wrap\n",
    "from kaitorch.core import Scalar\n",
    "\n",
    "__all__ = ['mse', 'binary_crossentropy', 'categorical_crossentropy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2d7f",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE) is often used for regression problems, where we are trying to predict a continuous and unbounded output value. It measures the average of the squared differences between the predicted output and the actual value for a given set of training data.\n",
    "\n",
    "__Breaking down MSE into it's respective terms in reverse:__\n",
    "\n",
    "- __Error__ - the error is the difference between the predicted output and the actual value\n",
    "\n",
    "- __Squared__ - squaring the error penalizes larger differences more heavily, making the model more likely to correct large errors during training.\n",
    "\n",
    "- __Mean__ - taking the mean of the squared error gives us a better estimate of the overall model prediction error (as compared to using only the error of a single sample), reducing the variance of our model training.\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $y_i$ is the actual value (continuous unbounded)\n",
    "- $\\hat{y}_i$ is the predicted value (continuous unbounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3434d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def mse():\n",
    "    return MeanSquaredError()\n",
    "\n",
    "class MeanSquaredError:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys: list, y_preds: list):\n",
    "\n",
    "        ys, y_preds = wrap(ys), wrap(y_preds)\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "        \n",
    "        # Summation Term\n",
    "        squared_error = sum(\n",
    "            (y - y_pred)**2 for y, y_pred in zip(ys, y_preds))\n",
    "        \n",
    "        # Mean Squared Error\n",
    "        mean_squared_error = squared_error/pred_length\n",
    "\n",
    "        return mean_squared_error\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'MeanSquaredError()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf998ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=0, y_pred=0  , MSE loss is 1e-16\n",
      "When y=0, y_pred=0.1, MSE loss is 0.01\n",
      "When y=0, y_pred=1  , MSE loss is 1.0\n",
      "When y=0, y_pred=10 , MSE loss is 100.0\n"
     ]
    }
   ],
   "source": [
    "mse = mse()\n",
    "\n",
    "ys = [0, 0, 0, 0]\n",
    "y_preds = [0, 0.1, 1, 10]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    mse_loss = mse(y, Scalar(y_pred))\n",
    "    print(f'When y={y}, y_pred={y_pred:<3}, MSE loss is {mse_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270b8b3",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy\n",
    "\n",
    "Binary Cross Entropy (BCE) is often used in binary classification, problems where we are trying to determine if a sample belongs to the positive (1) or negative (0) class. BCE is a measure of __how well a model is able to distinguish between the positive and negative class.__\n",
    "\n",
    "$$ \\text{BCE} = -\\frac{1}{N} \\sum_{i=1}^N [y_i * \\ln \\hat{y}_i + (1-y_i) * \\ln (1-\\hat{y}_i)] $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $y_i$ is the actual class (0 or 1)\n",
    "- $\\hat{y}_i$ is the predicted class (continuous between 0 and 1)\n",
    "\n",
    "Splitting this up into its left and right hand terms.\n",
    "\n",
    "__left term:__ $y_i * \\ln \\hat{y}_i$\n",
    "\n",
    "- When the actual class is positive ($y_i=1$), then $y_i = 1, 1 - y_i = 0$ and the right hand term is 0.\n",
    "\n",
    "__right term:__ $(1-y_i) * \\ln (1-\\hat{y}_i)$\n",
    "\n",
    "- When the actual class is negative ($y_i=0$), then $y_i = 0, 1 - y_i = 1$ and the left hand term is 0.\n",
    "\n",
    "So, for any data point, only half of this loss function is \"active\" (the other is 0), and it reduces to the negative logarithm of the predicted probability of the actual class.\n",
    "\n",
    "Negative logarithm is used because it has the following property:\n",
    "- When $x=1, \\ln(x)=0$, so when we predict the correct class, the error term is 0\n",
    "- When $x=0, \\ln(x)=\\infty$, so when we predict the incorrect class, the error term grows increasingly large\n",
    "\n",
    "By minimizing the BCE loss function, the model is encouraged to produce predicted probabilities that are closer to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0086915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def binary_crossentropy():\n",
    "    return BinaryCrossentropy()\n",
    "\n",
    "class BinaryCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys, y_preds):\n",
    "\n",
    "        loss = 0.0\n",
    "        ys, y_preds = wrap(ys), wrap(y_preds)\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "\n",
    "        # Summation term - could've done this more concisely but wanted to make the logic clear\n",
    "        for y, y_pred in zip(ys, y_preds):\n",
    "\n",
    "            # active left term\n",
    "            if y == 1:\n",
    "                loss += -(y_pred).log()\n",
    "\n",
    "            # active right term\n",
    "            elif y == 0:\n",
    "                loss += -(1 - y_pred).log()\n",
    "\n",
    "        # Binary Cross Entropy\n",
    "        binary_crossentropy_loss = loss / pred_length\n",
    "\n",
    "        return binary_crossentropy_loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BinaryCrossentropy()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c008ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=1, y_pred=0.99, BCE loss is 0.01005\n",
      "When y=1, y_pred=0.01, BCE loss is 4.6052\n",
      "When y=0, y_pred=0.99, BCE loss is 4.6052\n",
      "When y=0, y_pred=0.01, BCE loss is 0.01005\n"
     ]
    }
   ],
   "source": [
    "bce = BinaryCrossentropy()\n",
    "\n",
    "ys = [1, 1, 0, 0]\n",
    "y_preds = [0.99, 0.01, 0.99, 0.01]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    bce_loss = bce(y, Scalar(y_pred))\n",
    "    print(f'When y={y}, y_pred={y_pred}, BCE loss is {bce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058e165",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy\n",
    "\n",
    "Categorical Cross Entropy (CCE) is often used for multi-class classification, problems where we are trying to determine which class a sample belongs to. CCE is a measure of __how well a model is able to distinguish the correct class__.\n",
    "\n",
    "CCE is the generalized version of BCE - instead of determining whether or not a sample belongs to a class, it determines which, out of multiple classes, a sample belongs to.\n",
    "\n",
    "Class membership (or otherwise) is represented using __one-hot encodings__. For example, if we have 3 classes and a sample belongs to the second class, we represent this using the __one-hot vector__ `[0, 1, 0]`.\n",
    "\n",
    "$$ \\text{CCE} = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{j}^C [y_{ij} * \\ln \\hat{y}_{ij} + (1-y_{ij}) * \\ln (1-\\hat{y}_{ij})] $$\n",
    "\n",
    "- $N$ is the number of samples\n",
    "- $C$ is the number of classes\n",
    "- $y_i$ is the OHE vector of the actual class (list of 0s and 1s)\n",
    "- $y_{ij}$ is whether or not the actual class is `j` (list of 0s and 1s)\n",
    "- $\\hat{y}_i$ is the vector of the predcted classes (list of values between 0 and 1)\n",
    "- $\\hat{y}_{ij}$ is the predicted probability of class `j`\n",
    "\n",
    "In plain terms, this is how BCE generalizes to CCE:\n",
    "\n",
    "- With BCE, the question is whether or not a sample belongs to a class.\n",
    "- With CCE, the question is whether or not a sample belongs to class A, whether or not it belongs to class B, ... etc.\n",
    "\n",
    "Categorical cross entropy is essentially the sum of BCEs for every class in our problem, and as you'll see in the code below, the logic is essentially the same, with an added loop to account for the additional classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa2f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/losses.py\n",
    "\n",
    "def categorical_crossentropy():\n",
    "    return CategoricalCrossentropy()\n",
    "\n",
    "class CategoricalCrossentropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, ys, y_preds):\n",
    "\n",
    "        loss = 0.0\n",
    "        if isinstance(ys[0], (int, float, Scalar)):\n",
    "            ys, y_preds = [ys], [y_preds]\n",
    "\n",
    "        # 1/N\n",
    "        pred_length = len(ys)\n",
    "\n",
    "        # Outer summation term - again, this could've been more concise but wanted to make the logic clear\n",
    "        for y_ohe, y_pred_ohe in zip(ys, y_preds):\n",
    "\n",
    "            # Inner summation term\n",
    "            for y, y_pred in zip(y_ohe, y_pred_ohe):\n",
    "                \n",
    "                # if j is the actual class\n",
    "                if y == 1:\n",
    "                    loss += -(y_pred).log()\n",
    "                    \n",
    "                # if j is not the actual class\n",
    "                elif y == 0:\n",
    "                    loss += -(1 - y_pred).log()\n",
    "\n",
    "        # Categorical Cross Entropy\n",
    "        categorical_crossentropy_loss = loss / pred_length\n",
    "\n",
    "        return categorical_crossentropy_loss\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CategoricalCrossEntropy()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d24b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.99), Scalar(data=0.01), Scalar(data=0.01)], CCE loss is 0.030151\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.99), Scalar(data=0.01), Scalar(data=0.99)], CCE loss is 4.6253\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.01), Scalar(data=0.01), Scalar(data=0.99)], CCE loss is 9.2204\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.01), Scalar(data=0.99), Scalar(data=0.99)], CCE loss is 13.816\n"
     ]
    }
   ],
   "source": [
    "cce = CategoricalCrossentropy()\n",
    "\n",
    "ys = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]\n",
    "\n",
    "y_preds = [\n",
    "    [Scalar(0.99), Scalar(0.01), Scalar(0.01)],\n",
    "    [Scalar(0.99), Scalar(0.01), Scalar(0.99)],\n",
    "    [Scalar(0.01), Scalar(0.01), Scalar(0.99)],\n",
    "    [Scalar(0.01), Scalar(0.99), Scalar(0.99)]\n",
    "]\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    cce_loss = cce(y, y_pred)\n",
    "    print(f'When y={y}, y_pred={y_pred}, CCE loss is {cce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c3860",
   "metadata": {},
   "source": [
    "__if we wanted to__ (which we dont) __, we could just plug BCE into our CCE :)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70931f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, ys, y_preds):\n",
    "    \n",
    "    print(\"Using BCE\") # just to prove we're using this __call__\n",
    "\n",
    "    if isinstance(ys[0], (int, float, Scalar)):\n",
    "        ys, y_preds = [ys], [y_preds]\n",
    "\n",
    "    # 1/N\n",
    "    pred_length = len(ys)\n",
    "\n",
    "    # Outer summation term\n",
    "    for y_ohe, y_pred_ohe in zip(ys, y_preds):\n",
    "\n",
    "        categorical_crossentropy_loss = 3*BinaryCrossentropy()(y_ohe, y_pred_ohe) \n",
    "        # add 3* since there's no 1/C term   ^^ here lol ^^ \n",
    "\n",
    "    # Categorical Cross Entropy\n",
    "    categorical_crossentropy_loss = categorical_crossentropy_loss / pred_length\n",
    "\n",
    "    return categorical_crossentropy_loss\n",
    "\n",
    "CategoricalCrossentropy.__call__ = __call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8be87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.99), Scalar(data=0.01), Scalar(data=0.01)], CCE loss is 0.030151\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.99), Scalar(data=0.01), Scalar(data=0.99)], CCE loss is 4.6253\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.01), Scalar(data=0.01), Scalar(data=0.99)], CCE loss is 9.2204\n",
      "Using BCE\n",
      "When y=[1, 0, 0], y_pred=[Scalar(data=0.01), Scalar(data=0.99), Scalar(data=0.99)], CCE loss is 13.816\n"
     ]
    }
   ],
   "source": [
    "# Verify we get the same result\n",
    "\n",
    "cce = CategoricalCrossentropy()\n",
    "\n",
    "for y, y_pred in zip(ys, y_preds):\n",
    "    cce_loss = cce(y, y_pred)\n",
    "    print(f'When y={y}, y_pred={y_pred}, CCE loss is {cce_loss.data:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c7f09",
   "metadata": {},
   "source": [
    "__Those are our 3 loss functions in KaiTorch!__ Now that we have proper loss functions, let's continue building functionality around our `Sequential` class.\n",
    "\n",
    "This is where we left off last notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a1db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaitorch.core import Module\n",
    "from kaitorch.layers import Dense\n",
    "from kaitorch.utils import unwrap\n",
    "\n",
    "class Sequential(Module):\n",
    "\n",
    "    def __init__(self, layers=None):\n",
    "        self.built = False\n",
    "\n",
    "        self.layers = layers if layers else []\n",
    "        self.layer_sizes = [\n",
    "            layer.nouts for layer in self.layers\n",
    "            ] if self.layers else []\n",
    "    \n",
    "    def __repr__(self):\n",
    "        print([layer.parameters() for layer in self.layers])\n",
    "        return '\\n'.join(str(layer) for layer in self.layers)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return unwrap(x)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.layer_sizes.append(layer.nouts)\n",
    "    \n",
    "    def build(self, input_size):\n",
    "\n",
    "        if self.built:\n",
    "            return\n",
    "\n",
    "        self.layer_sizes.insert(0, input_size)\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            layer.__build__(self.layer_sizes[idx])\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def summary(self):\n",
    "        print('_' * 115)\n",
    "        print('Layer (params)' + ' '*59 + 'Output Shape' + ' '*5 + 'Params = Weights + Biases')\n",
    "        print('=' * 115)\n",
    "        for layer_num, layer in enumerate(self.layers):\n",
    "            l_name = layer.__repr__()\n",
    "            l_output = f'(None, {layer.nouts})'\n",
    "            l_params = len(layer.parameters())\n",
    "            l_w = l_params - layer.nouts if l_params > 0 else 0\n",
    "            l_b = layer.nouts if l_params > 0 else 0\n",
    "\n",
    "            print(f'{l_name:<73}{l_output:<17}{l_params:<9}{l_w:<10}{l_b:<6}')\n",
    "            if layer_num != (len(self.layers) - 1):\n",
    "                print('_' * 115)\n",
    "        print('=' * 115)\n",
    "        print(f'Total Params: {sum([len(layer.parameters()) for layer in self.layers])}')\n",
    "        print('_' * 115)\n",
    "\n",
    "    def plot(self, filename=None):\n",
    "\n",
    "        if not self.built:\n",
    "            raise Exception('[Model Not Built] - Use Sequential.build(input_size) to build model')\n",
    "        empty_input = self.__call__([0]*self.layer_sizes[0])\n",
    "        return plot_model(empty_input, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a48513",
   "metadata": {},
   "source": [
    "First, let's define the `Module` class that `Sequential` inherits that I introduced last notebook. Every layer and model will inherit this class that contains the `zero_grad` method that should look familiar (step 3 of our training loop) and a `parameter` method as a reminder that this method should be implemented for every `Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86c81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/core.py\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfae48",
   "metadata": {},
   "source": [
    "Revisiting our manual training loop from last notebook, let's start with the idea that we don't want to train the model every time we run it, let's introduce a new parameter `train` when we call our model. Its currently unused in `__call__` but we'll need it in Notebook 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed771c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x, train):\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    return unwrap(x)\n",
    "\n",
    "Sequential.__call__ = __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd73ae5",
   "metadata": {},
   "source": [
    "Let's incorporate our work on loss functions. We'll add a `compiled` property to `Sequential` and implement a `compile` method that sets the loss function of the model (as well as the optimizer, but that's next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87535c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/models.py\n",
    "\n",
    "def __init__(self, layers=None):\n",
    "    self.built = False\n",
    "    self.compiled = False\n",
    "\n",
    "    self.layers = layers if layers else []\n",
    "    self.layer_sizes = [layer.nouts for layer in self.layers] if self.layers else []\n",
    "    \n",
    "Sequential.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc01d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaitorch.losses\n",
    "\n",
    "def compile(self, loss):\n",
    "\n",
    "    # kaitorch/models.py\n",
    "    def set_loss(loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss in kaitorch.losses.__all__:\n",
    "                self.loss = getattr(kaitorch.losses, loss)()\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f'[Undefined Loss Function] - Loss Function \"{loss}\" has not been implemented'\n",
    "                )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    if not self.compiled:\n",
    "        if loss:\n",
    "            set_loss(loss)\n",
    "            self.compiled = True\n",
    "        else:\n",
    "            raise Exception(\n",
    "                '[Unable to Compile] - Optimizer and Loss Function must be specified'\n",
    "            )\n",
    "            \n",
    "Sequential.compile = compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519429f",
   "metadata": {},
   "source": [
    "Next, let's write each iteration of the training loop from last notebook into a method `run_epoch`, and separate the functions that are only needed for training into an if statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2336f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_epoch(self, x, y=None, epoch=1, epochs=1, train=False):\n",
    "\n",
    "    postfix_type = 'Train' if train is True else ''\n",
    "\n",
    "    # Progress bar looping through all inputs\n",
    "    tqdm_x = tqdm(\n",
    "        x,\n",
    "        ncols=160,\n",
    "        desc=f\"Epoch {epoch:>3}/{epochs}\", \n",
    "        postfix='',\n",
    "        bar_format='{l_bar}{bar:40}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]'\n",
    "    )\n",
    "\n",
    "    # List to store model predictions\n",
    "    y_pred = []\n",
    "\n",
    "    # For every input\n",
    "    for x_record in tqdm_x:\n",
    "        \n",
    "        # Replacing [model(x) for x in xs]\n",
    "        y_pred.append(self.__call__(x_record, train=train))\n",
    "\n",
    "        # If an true y value is provided, calculate the loss value and display in progress bar\n",
    "        if y:\n",
    "            run_loss = self.loss(y[:len(y_pred)], y_pred)\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type} Loss: {run_loss.data:.4f}\")\n",
    "\n",
    "        # Else, loss value is None, and do not display in progress bar\n",
    "        else:\n",
    "            run_loss = None\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type}\")\n",
    "\n",
    "    # If this is a training run, \n",
    "    if train:\n",
    "        self.zero_grad() # using zero_grad() inherited from Module\n",
    "        run_loss.backward()\n",
    "        self.step() # we haven't defined this yet\n",
    "\n",
    "    return y_pred, run_loss\n",
    "\n",
    "Sequential.run_epoch = run_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1ab86",
   "metadata": {},
   "source": [
    "We haven't defined the `step()` method yet, which has a dependency on which optimizer we use, but for now we'll use the same simple gradient descent algorithm we used last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2648320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, **optimizer_params):\n",
    "\n",
    "    for p in self.parameters():\n",
    "        p.data += 0.1 * -1 * p.grad\n",
    "        \n",
    "Sequential.step = step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38270ce",
   "metadata": {},
   "source": [
    "### Updated Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc094ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________________________\n",
      "Layer (params)                                                           Output Shape     Params = Weights + Biases\n",
      "===================================================================================================================\n",
      "Dense(units=3, activation=tanh, initializer=he_uniform)                  (None, 3)        12       9         3     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=4, activation=tanh, initializer=he_uniform)                  (None, 4)        16       12        4     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=4, activation=tanh, initializer=he_uniform)                  (None, 4)        20       16        4     \n",
      "___________________________________________________________________________________________________________________\n",
      "Dense(units=1, activation=tanh, initializer=glorot_uniform)              (None, 1)        5        4         1     \n",
      "===================================================================================================================\n",
      "Total Params: 53\n",
      "___________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(3, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(4, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(4, activation='tanh', initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "model.compile(loss='mse')\n",
    "model.build(3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ae8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[ 2.0,  3.0, -1.0],\n",
    "      [ 3.0, -1.0,  0.5],\n",
    "      [-0.5,  1.0,  1.0],\n",
    "      [ 1.0,  1.0, -1.0],\n",
    "      [ 2.5, -1.0, -1.0]]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0, -1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec093e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch   0/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 1.4682]\n",
      "Epoch   1/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 1.1969]\n",
      "Epoch   2/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.8670]\n",
      "Epoch   3/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.8156]\n",
      "Epoch   4/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.8044]\n",
      "Epoch   5/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.7933]\n",
      "Epoch   6/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.7746]\n",
      "Epoch   7/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.7258]\n",
      "Epoch   8/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.5894]\n",
      "Epoch   9/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.4321]\n",
      "Epoch  10/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.2450]\n",
      "Epoch  11/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.1392]\n",
      "Epoch  12/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0973]\n",
      "Epoch  13/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0742]\n",
      "Epoch  14/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0583]\n",
      "Epoch  15/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0467]\n",
      "Epoch  16/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0381]\n",
      "Epoch  17/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0315]\n",
      "Epoch  18/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0264]\n",
      "Epoch  19/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0224]\n",
      "Epoch  20/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0193]\n",
      "Epoch  21/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0167]\n",
      "Epoch  22/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0146]\n",
      "Epoch  23/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0129]\n",
      "Epoch  24/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0114]\n",
      "Epoch  25/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0102]\n",
      "Epoch  26/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0092]\n",
      "Epoch  27/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0083]\n",
      "Epoch  28/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0075]\n",
      "Epoch  29/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0068]\n",
      "Epoch  30/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0062]\n",
      "Epoch  31/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0057]\n",
      "Epoch  32/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0053]\n",
      "Epoch  33/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0049]\n",
      "Epoch  34/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0045]\n",
      "Epoch  35/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0042]\n",
      "Epoch  36/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0039]\n",
      "Epoch  37/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0036]\n",
      "Epoch  38/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0034]\n",
      "Epoch  39/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0032]\n",
      "Epoch  40/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0030]\n",
      "Epoch  41/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0028]\n",
      "Epoch  42/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0026]\n",
      "Epoch  43/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0025]\n",
      "Epoch  44/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0023]\n",
      "Epoch  45/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0022]\n",
      "Epoch  46/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0021]\n",
      "Epoch  47/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0020]\n",
      "Epoch  48/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0019]\n",
      "Epoch  49/50: 100%|████████████████████████████████████████| 5/5 [00:00<00:00, Train Loss: 0.0018]\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    y_pred, run_loss = model.run_epoch(xs, ys, epoch, epochs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55292800",
   "metadata": {},
   "source": [
    "Cleaner, right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
