{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ea8072",
   "metadata": {},
   "source": [
    "# Notebook 07 - Inverted Dropout\n",
    "\n",
    "We now have a fully functioning library that can make multi-layer perceptrons - a neural net that is a series of Dense layers. [edit]\n",
    "\n",
    "One of the big problems deep neural networks face, as they can be highly overparametrized, is overfitting and lack of generalizability, causing it to perform poorly on unseen data. In the final \"tutorial\" notebook of this series, I wanted to introduce and implement a technique used to combat this issue - __dropout__.\n",
    "\n",
    "The idea behind dropout is actually quite simple, but there are some nuances to cover and intuitions to develop.\n",
    "\n",
    "Dropout works by randomly \"dropping out\" (lol) a number of neurons in a neural network during each training iteration at a user-defined rate. When a neuron is dropped out, the neuron along with its connections are not used in the forward or backword pass. This means that in each training iterations, different sets of neurons are active, which forces the network to be more robust and prevent over-reliance on any group of neurons.\n",
    "\n",
    "Conceptually, some like to develop the intuition that using dropout approximates training a large number of neural networks with slightly different architectures at the same time.\n",
    "\n",
    "[explain and add diagrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f671d",
   "metadata": {},
   "source": [
    "# Dropout Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9a0dc",
   "metadata": {},
   "source": [
    "The only hyperparameter we'll implement in our `Dropout` layer is `dropout_rate` - the probability that any given neuron is activated.\n",
    "\n",
    "We'll then define a parameter in each node `p`, which is the `p`robability that a neuron will be activated, this is calculated as `1 - dropout_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5e0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, dropout_rate: float):\n",
    "        self.p = 1 - dropout_rate\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7124fb",
   "metadata": {},
   "source": [
    "Then, we generate a random float between 0 and 1 - there will be a `p` chance that this number is less than `p`, and a `1-p` chance that the number is greater than `p`. Therefore, if the number is lesser than p, we will allow the neuron to activate, if not, it will be masked and set to 0.\n",
    "\n",
    "However, let's say we have a dropout_rate of 0.5, implying only half of the nodes will be active - this will result in  the magnitude of values being propogated forward in the model to approximately halve, which [ explain why this is stupid ].\n",
    "\n",
    "So, to offset this unwanted effect of dropout, we multiply active weights by `1/self.p`. This will help us preserve the magnitude of the total values being propogated forward and allow the model to [do dropout stuff]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac79d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, x, train):\n",
    "    return x * (1/self.p) if random.random() <= self.p else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e065fc",
   "metadata": {},
   "source": [
    "And finally, we only want dropout to be applied during model training to take advantage of the regularization effects. When we are using the model to make predictions, we want to make use of all the trained weights of the full model, which are [more regularized?] and less likely to be overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37fd4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "def __call__(self, x, train):\n",
    "    if train:\n",
    "        return x * (1/self.p) if random.random() <= self.p else 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "Node.__call__ = __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c4f47",
   "metadata": {},
   "source": [
    "# Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9c2ca",
   "metadata": {},
   "source": [
    "As mentioned earlier, the only hyperparameter we need to tune is dropout_rate. As with our dense layer, we also need to define `nins` (number of inputs), `nouts` (number of outputs), and `nodes` (list of dropout nodes). \n",
    "\n",
    "It's pretty trivial to define nins and nouts they will be identical (each input gets its own dropout Node, so the size of the dropout layer will be the same as the previous layer), but for consistency and clarity we'll add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bf8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "from kaitorch.core import Module\n",
    "\n",
    "class Dropout(Module):\n",
    "\n",
    "    def __init__(self, dropout_rate: float=0.5):\n",
    "\n",
    "        self.nins = None\n",
    "        self.nouts = None\n",
    "        self.nodes = None\n",
    "        self.p = dropout_rate\n",
    "\n",
    "        if self.p < 0 or self.p > 1:\n",
    "            raise ValueError(\"p must be a probability\")\n",
    "            \n",
    "    def parameters(self):\n",
    "        return [p for node in self.nodes for p in node.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Dropout(dropout_rate={self.p})'\n",
    "    \n",
    "Dropout.Node = Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85541ce",
   "metadata": {},
   "source": [
    "Since `nins` is entirely dependent on the output size of the previous layer, we will initialize this as `None`, and fill in this value after we define the architecture of our model and `__build__` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30c35b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "def __build__(self, nins):\n",
    "    self.nins = nins\n",
    "    self.nouts = nins\n",
    "    self.nodes = [self.Node(self.p) for _ in range(self.nins)]\n",
    "    \n",
    "Dropout.__build__ = __build__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ab20e",
   "metadata": {},
   "source": [
    "The last thing to do is implement the `__call__` method, which takes in the output of the previous layer as an input and calls the dropout node.\n",
    "\n",
    "Unlike the Dense layer, we also need to pass in the `train` boolean parameter as we want [different performance lol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bac3ab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "def __call__(self, xs, train):\n",
    "    outs = [n(xi, train) for n, x in zip(self.nodes, xs)]\n",
    "    return unwrap(outs)\n",
    "\n",
    "Dropout.__call__ = __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a17e3",
   "metadata": {},
   "source": [
    "# Finishing our Sequential Class\n",
    "\n",
    "We're almost done building KaiTorch! There's 2 things left on our agenda before we wrap up.\n",
    "\n",
    " - 1) Update our methods to incorporate the `Dropout` layer\n",
    " - 2) Build out methods similar to Keras for interfacing with the model (`fit, predict, evaluate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaaaa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaitorch.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258d6ca",
   "metadata": {},
   "source": [
    "As mentioned above, unlike the Dense layer, calling the Dropout layer requires a `train` argument to specify if the current iteration is for training or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "def __call__(self, x, train):\n",
    "    for layer in self.layers:\n",
    "        if isinstance(layer, Dropout):\n",
    "            x = layer(x, train)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "    return unwrap(x)\n",
    "\n",
    "Sequential.__call__ = __call__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f839c",
   "metadata": {},
   "source": [
    "Also mentioned above, the `nins` of a Dropout layer is initialized as `None` as it depends on the output size of the previous layer. So, when building our model, we need to apply a forward fill on the list of layer sizes to fill in the size of each Dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "169a14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/utils.py\n",
    "\n",
    "def ffill(x: list):\n",
    "    for i in range(1, len(x)-1):\n",
    "        if x[i] is None:\n",
    "            x[i] = x[i-1]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da6adce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "def build(self, input_size):\n",
    "\n",
    "    if self.built:\n",
    "        return\n",
    "\n",
    "    self.layer_sizes.insert(0, input_size)\n",
    "    self.layer_sizes = ffill(self.layer_sizes)\n",
    "\n",
    "    for idx, layer in enumerate(self.layers):\n",
    "        layer.__build__(self.layer_sizes[idx])\n",
    "\n",
    "    self.built = True\n",
    "    \n",
    "Sequential.build = build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372961d",
   "metadata": {},
   "source": [
    "Next, we'll define a `run` method that `fit`, `predict`, and `evaluate` will all be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13832d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/layers.py\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run(self, x, y=None, epoch=1, epochs=1, train=False):\n",
    "\n",
    "    # This code just prints the progress bar - it's also the only external library used in KaiTorch\n",
    "    postfix_type = 'Train' if train is True else ''\n",
    "\n",
    "    tqdm_x = tqdm(\n",
    "        x,\n",
    "        ncols=160,\n",
    "        desc=f\"Epoch {epoch:>3}/{epochs}\", \n",
    "        postfix='',\n",
    "        bar_format='{l_bar}{bar:40}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]'\n",
    "    )\n",
    "\n",
    "    # Used to the predictions for every record in x\n",
    "    y_pred = []\n",
    "\n",
    "    # For every record in our dataset\n",
    "    for x_record in tqdm_x:\n",
    "\n",
    "        # Run a forward pass of the model\n",
    "        y_pred.append(self.__call__(x_record, train=train))\n",
    "\n",
    "        # If a y label is provided, calculate the loss value\n",
    "        if y:\n",
    "            run_loss = self.loss(y[:len(y_pred)], y_pred)\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type} Loss: {run_loss.data:.4f}\")\n",
    "        # Otherwise, don't\n",
    "        else:\n",
    "            run_loss = None\n",
    "            tqdm_x.set_postfix_str(f\"{postfix_type}\")\n",
    "\n",
    "    # This is the training loop introduced in Notebook 4\n",
    "    if train:\n",
    "        self.zero_grad()\n",
    "        run_loss.backward()\n",
    "        self.step()\n",
    "\n",
    "    return y_pred, run_loss\n",
    "\n",
    "Sequential.run = run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b199b3",
   "metadata": {},
   "source": [
    "Almost done! We just need to implement fit, predict, and evaluate, which have some overlapping steps. This is a summary of what each one needs to do:\n",
    "\n",
    "\n",
    "|  | fit() | predict() | evaluate() |\n",
    "| --- | --- | --- | --- |\n",
    "| Calculate Loss | Yes | No | Yes|\n",
    "| Output Predictions | No | Yes | No |\n",
    "| Train Model | Yes | No | No |\n",
    "\n",
    "<center><i>[i just learnt you could make tables like this with markdown pretty cool right]</i></center>\n",
    "\n",
    "So going row by row:\n",
    "\n",
    "- __Calculate Loss__ is returned by `run()`, and we need to initialize `history` to store the loss\n",
    "- __Output Predictions__ is also returned by `run()`, and we need to do some processing to make sure the output is in the format we want\n",
    "- __Train Model__ just requires us to pass `train=True` to `run()`, as well as the current `epoch` and number of `epochs` for printing and training\n",
    "\n",
    "All 3 functions will also need to apply `wrap` to our input to make sure it is an iterable, and will `build` the model if it hasn't been built yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdf93e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaitorch/models.py\n",
    "\n",
    "def fit(self, x, y, epochs=1):\n",
    "\n",
    "    x = wrap(x)\n",
    "    self.build(len(x[0]))\n",
    "\n",
    "    history = {'loss': []}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        y_pred, run_loss = self.run(x, y, epoch, epochs, train=True)\n",
    "        history['loss'].append(run_loss.data)\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate(self, x, y):\n",
    "\n",
    "    x = wrap(x)\n",
    "    self.build(len(x[0]))\n",
    "    \n",
    "    history = {'loss': []}\n",
    "\n",
    "    y_pred, run_loss = self.run(x, y)\n",
    "    history['loss'].append(run_loss.data)\n",
    "\n",
    "    return history\n",
    "\n",
    "def predict(self, x, as_scalar=False):\n",
    "\n",
    "    x = wrap(x)\n",
    "    self.build(len(x[0]))\n",
    "\n",
    "    y_pred, run_loss = self.run(x)\n",
    "\n",
    "    # as_scalar argument for user to specify if the output should be `Scalar` or numeric\n",
    "    if as_scalar:\n",
    "        return y_pred\n",
    "    else:\n",
    "        # if output of each run is a single Scalar | eg. binary classification\n",
    "        if isinstance(y_pred[0], Scalar):\n",
    "            return [y.data for y in y_pred]\n",
    "        # if output of each run is a list of Scalars | eg. multi-class classification\n",
    "        elif isinstance(y_pred[0][0], Scalar):\n",
    "            return [[y.data for y in row] for row in y_pred]\n",
    "\n",
    "Sequential.fit = fit\n",
    "Sequential.evaluate = evaluate\n",
    "Sequential.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ebac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
